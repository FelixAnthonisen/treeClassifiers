{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee = pd.read_csv('datasets/coffee_data.csv')\n",
    "wine = pd.read_csv('datasets/wine_dataset_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this project, we explored the use of decision trees and random forest classifiers for predicting the type or country of origin of coffee and wine, using a dataset containing relevant features. Decision trees, known for their simplicity and interpretability, were implemented alongside the more robust and ensemble-based random forest approach. To improve model performance, we conducted hyperparameter tuning using grid search, aiming to find the optimal configurations for each classifier. The final performance of our custom implementations was then compared against the standard versions provided by scikit-learn, to assess their effectiveness and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis\n",
    "\n",
    "The data consists of 2 datasets, one for coffee and one for wine. \n",
    "\n",
    "### Coffee dataset\n",
    "\n",
    "The available features for coffee are aroma, flavor, aftertaste, body, acidity, balance, uniformity and sweetness.\n",
    "\n",
    "These features appear to have a somewhat normal distribution, with most scores concentrated in the middle ranges (between 6 and 8). Slight skewness is visible for some attributes like Acidity and Balance, where most values are towards the higher range.\n",
    "\n",
    "For uniformity and sweetness the distributions are highly skewed with most of the data points are concentrated at 10.\n",
    "\n",
    "The target feature for this dataset is country of origin. The targets are relatively balanced with a slight skew towards 1. This skew might impact the performance of our classifiers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = coffee.columns\n",
    "\n",
    "# Create a grid of subplots\n",
    "n = len(features)\n",
    "ncols = 2  # Number of columns in the grid\n",
    "nrows = (n + ncols - 1) // ncols  # Number of rows required\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5 * nrows))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes into 1D\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axes[i].hist(coffee[feature], bins=20)\n",
    "    axes[i].set_title(f'Frequency of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'coffee' is your DataFrame\n",
    "corr_matrix = coffee.corr()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "\n",
    "# Save the figure as a PNG\n",
    "# plt.savefig(\"correlation_matrix.png\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset\n",
    "\n",
    "The available features for wine are citric acid, sulphates,residual sugar, pH and alcohol.\n",
    "\n",
    "Citric Acid, Sulphates, and Residual Sugar show irregular or skewed distributions, meaning certain characteristics might be rare or highly specific to certain wines.\n",
    "\n",
    "The dataset seems relatively balanced for pH and alcohol.\n",
    "\n",
    "The target feature for this dataset is type. The targets are balanced, which should be good for classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = wine.columns\n",
    "\n",
    "# Create a grid of subplots\n",
    "n = len(features)\n",
    "ncols = 2  # Number of columns in the grid\n",
    "nrows = (n + ncols - 1) // ncols  # Number of rows required\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 5 * nrows))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes into 1D\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axes[i].hist(wine[feature], bins=20)\n",
    "    axes[i].set_title(f'Frequency of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'coffee' is your DataFrame\n",
    "corr_matrix = wine.corr()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "\n",
    "# Save the figure as a PNG\n",
    "plt.savefig(\"correlation_matrix.png\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We decided that further data preprocessing was unnecessary. Before applying the decision tree and random forest classifiers, we reviewed the datasets and determined that the data was already in a form that could be directly utilized by these algorithms. The key reasons are outlined below:\n",
    "\n",
    "__Clean Data__:\n",
    "Both the coffee and wine datasets appeared to be free of missing values or outliers that could heavily skew the models. Decision trees and random forests are relatively robust to outliers, as they focus on partitioning the feature space based on thresholds, so no preprocessing was required to handle potential noise in the data.\n",
    "\n",
    "**Balanced Target Variables**:\n",
    "As shown in the histograms, the target variables (Country of Origin for coffee and Type for wine) were relatively balanced, with both classes well-represented in the datasets. As a result, we did not need to do any processing here.\n",
    "\n",
    "__Handling of Skewed Features__:\n",
    "While some features showed slight skewness (e.g., residual sugar in wine, uniformity in coffee), the decision tree-based models used (both decision trees and random forests) are non-parametric. These models do not assume a normal distribution in the data, making them robust to skewness and non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations\n",
    "\n",
    "### Decision tree classifier:\n",
    "\n",
    "We used the decision tree classifier to predict the type / country of origin of coffee and wine. The user is able to decide between a range of hyperparameters to tune the model:\n",
    "\n",
    "| Hyperparameter | Description | Default value | Possible values |\n",
    "| --- | --- | --- | --- |\n",
    "| `max_depth` | The maximum depth of the tree | `None` | `None` or `int` |\n",
    "| `criterion` | The function to measure the quality of a split | `gini` | `gini`, `entropy` |\n",
    "| `max_features` | The maximum amount of features | `None` | `log2`, `sqrt` and `None` |\n",
    "| `random_state` | The seed for all random functions | `0` | `int` |\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "The decision tree classifier `fit` method operates by recursively splitting the dataset into subsets based on the feature that yields the most informative split. This process continues until the data is fully partitioned into subsets, effectively creating a tree structure. \n",
    "\n",
    "To evaluate the quality of each split, the classifier employs metrics such as Gini impurity or entropy to calculate the information gain we would get if this split was performed. More\n",
    "\n",
    "Key parameters in the decision tree algorithm include:\n",
    "\n",
    "max_depth: This parameter limits the depth of the tree, ensuring that no node exceeds a specified level. This helps prevent overfitting by controlling the complexity of the model. Formally, for any node $n$, the condition $n_{depth} \\leq \\text{max\\char`_depth} $ must hold\n",
    "\n",
    "max_features: This parameter specifies the maximum number of features to consider when searching for the best split at each node. By limiting the features, the algorithm introduces randomness, which can enhance model generalization.\n",
    "\n",
    "random_state: This parameter serves as a seed for any random processes within the algorithm, ensuring reproducibility of results across different runs.\n",
    "\n",
    "\n",
    "**Infomation gain (IG)**\n",
    "\n",
    "We calculate IG for each feature and choose the feature with the highest IG to split the data on. The IG is calculated as follows:\n",
    "\n",
    "$$IG(x, y) = H(y) - H(y|x)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $IG(x, y)$ is the information gain\n",
    "\n",
    "- $H(y)$ is the entropy/gini index of the target variable\n",
    "\n",
    "- $H(y|x)$ is the conditional entropy/gini index of the target variable given the feature\n",
    "\n",
    "**Prediction process**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest classifier:\n",
    "\n",
    "We built the RFC on top of the decision tree classifier. The way it works is that it uses bagging. This means that it trains multiple decision trees on different subsets of the data to reduce overfitting and increase diversity among the trees. The RFC uses the majority vote to make predictions. The following hyperparameters can be tuned:\n",
    "\n",
    "| Hyperparameter | Description | Default value | Possible values |\n",
    "| --- | --- | --- | --- |\n",
    "| `n_estimators` | The number of trees to train | `100` | `int` |\n",
    "| `max_depth` | The maximum depth of the tree | `None` | `None` or `int` |\n",
    "| `criterion` | The function to measure the quality of a split | `gini` | `gini`, `entropy` |\n",
    "| `max_features` | The maximum amount of features | `sqrt` | `log2`, `sqrt` and `None` |\n",
    "| `random_state` | The seed for all random functions | `0` | `int` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and evaluation\n",
    "\n",
    "We tuned the aforementioned hyperparameters using grid search. We use accuracy as the metric to evaluate the model. The reason why is because it is a simple metric that is easy to understand and works quite well, although it is prone to overfitting. However, this was mitigated by using cross-validation (from `sklearn GridSearchCV`) as well as using a random forest classifier (majority vote).\n",
    "\n",
    "We need to figure out which model is the best for each dataset. The following steps will be taken:\n",
    "\n",
    "- Split the data into a training and test set\n",
    "\n",
    "- Tune hyperparameters for both models using grid search\n",
    "\n",
    "- Evaluate the models using accuracy\n",
    "\n",
    "- Choose the best model for each dataset\n",
    "\n",
    "\n",
    "### Hyperparameter tuning for the decision tree classifier\n",
    "\n",
    "| Hyperparameter | Potential values |\n",
    "| --- | --- |\n",
    "| `max_depth` | `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, None]` |\n",
    "| `criterion` | `[\"entropy\", \"gini\"]` |\n",
    "| `max_features` | `[\"log2\", \"sqrt\", None]` |\n",
    "\n",
    "### Hyperparameter tuning for the random forest classifier\n",
    "\n",
    "| Hyperparameter | Potential values |\n",
    "| --- | --- |\n",
    "| `n_estimators` | `[5, 6, 7, 8, 9, 10, 25, 30, 50, 75, 100]` |\n",
    "| `max_depth` | `[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, None]` |\n",
    "| `criterion` | `[\"entropy\", \"gini\"]` |\n",
    "| `max_features` | `[\"log2\", \"sqrt\", None]` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine dataset\n",
    "\n",
    "\n",
    "We tune the hyperparameters by using Grid Search Cross Validation. We use the following hyperparameters: \n",
    "\n",
    "\n",
    "\n",
    "### Our decision tree classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `max_depth` | `None` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `None` |\n",
    "\n",
    "### Our random forest classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `n_estimators` | `100` |\n",
    "| `max_depth` | `5` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `sqrt` |\n",
    "\n",
    "### SKLearn decision tree classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `max_depth` | `7` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `None` |\n",
    "\n",
    "### SKLearn random forest classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `n_estimators` | `75` |\n",
    "| `max_depth` | `9` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `log2` |\n",
    "\n",
    "Our decision tree classifier best model score: Training = 1, validation = 0.84\n",
    "\n",
    "Our random forest classifier best model score: Training = 0.9971, validation = 0.87333\n",
    "\n",
    "SKLearn decision tree classifier best model score: Training = 0.9485, validation = 0.7866\n",
    "\n",
    "SKLearn random forest classifier best model score: Training = 0.9971, validation = 0.87333\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coffee dataset\n",
    "\n",
    "\n",
    "\n",
    "We tune the hyperparameters by using Grid Search Cross Validation. We use the following hyperparameters: \n",
    "\n",
    "### Our decision tree classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `max_depth` | `None` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `None` |\n",
    "\n",
    "### Our random forest classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `n_estimators` | `100` |\n",
    "| `max_depth` | `5` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `sqrt` |\n",
    "\n",
    "### SKLearn decision tree classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `max_depth` | `3` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `None` |\n",
    "\n",
    "### SKLearn random forest classifier: \n",
    "\n",
    "| Hyperparameter | Value |\n",
    "| --- | --- |\n",
    "| `n_estimators` | `25` |\n",
    "| `max_depth` | `4` |\n",
    "| `criterion` | `entropy` |\n",
    "| `max_features` | `None` |\n",
    "\n",
    "Our decision tree classifier best model score: Training = 1, validation = 0.73\n",
    "\n",
    "Our random forest classifier best model score: Training = 0.86, validation = 0.8015\n",
    "\n",
    "SKLearn decision tree classifier best model score: Training = 0.8464, validation = 0.7936\n",
    "\n",
    "SKLearn random forest classifier best model score: Training = 0.8737, validation = 0.8095\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Comparing the results, we can see that the random forest classifier is the best model for both datasets. The random forest classifier has a higher accuracy than the decision tree classifier. The random forest classifier is able to reduce overfitting by training multiple decision trees on different subsets of the data. We also observe that the SKLearn implementation of the RFC has a similar accuracy to our implementation. This shows that our implementation is more or less correct because SKLearn's implementation is well-tested and optimized. This applies to both datasets. \n",
    "\n",
    "We have successfully created a decision tree classifier and a random forest classifier to predict the type / country of origin of coffee and wine. We have tuned the hyperparameters for both models using grid search. We have evaluated the models using accuracy. We have chosen the best model for each dataset.\n",
    "\n",
    "For next time it could be wise to clean the datasets to remove any statistical outliers in order to improve the accuracy of the models. We could also try to use other models to see if we can get better results. Having a wider spread of hyperparameters could also be beneficial. We chose not to do that because it would significantly worsen computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
