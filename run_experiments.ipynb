{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 â€“ Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload all modules without having to restart the kernel\n",
    "# Useful for development if you have edited any of the external code files.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# ... add more imports as needed\n",
    "\n",
    "# My implementations\n",
    "from decision_tree import DecisionTree\n",
    "from random_forest import RandomForest\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from itertools import product\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Do data loading, exploration and preprocessing as you see fit.\n",
    "\n",
    "Here is some code to load the dataset to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns names: ['citric_acid', 'residual_sugar', 'pH', 'sulphates', 'alcohol']\n",
      "Target column name: type\n",
      "X shape: (500, 5)\n",
      "y shape: (500,)\n",
      "[[ 0.13  1.6   3.34  0.59  9.2 ]\n",
      " [ 0.1   2.8   3.6   0.66 10.2 ]\n",
      " [ 0.32  1.9   3.2   0.55  9.5 ]\n",
      " ...\n",
      " [ 0.36  7.    3.04  0.32 11.  ]\n",
      " [ 0.42  2.1   3.11  0.78 12.4 ]\n",
      " [ 0.15  9.7   3.05  0.3   9.1 ]]\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"datasets/wine_dataset_small.csv\", delimiter=\",\", dtype=float, names=True)\n",
    "\n",
    "feature_names = list(data.dtype.names[:-1])\n",
    "target_name = data.dtype.names[-1]\n",
    "\n",
    "X = np.array([data[feature] for feature in feature_names]).T\n",
    "y = data[target_name].astype(int)\n",
    "\n",
    "print(f\"Feature columns names: {feature_names}\")\n",
    "print(f\"Target column name: {target_name}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "Estimators:  1\n",
      "2.314814814814815%\n",
      "Estimators:  5\n",
      "4.62962962962963%\n",
      "Estimators:  20\n",
      "6.944444444444445%\n",
      "Estimators:  5\n",
      "9.25925925925926%\n",
      "Estimators:  10\n",
      "11.574074074074074%\n",
      "Estimators:  1\n",
      "13.88888888888889%\n",
      "Estimators:  10\n",
      "16.203703703703702%\n",
      "Estimators:  20\n",
      "18.51851851851852%\n",
      "Estimators:  5\n",
      "20.833333333333336%\n",
      "Estimators:  20\n",
      "23.14814814814815%\n",
      "Estimators:  1\n",
      "25.462962962962965%\n",
      "Estimators:  10\n",
      "27.77777777777778%\n",
      "Estimators:  1\n",
      "30.09259259259259%\n",
      "Estimators:  5\n",
      "32.407407407407405%\n",
      "Estimators:  20\n",
      "34.72222222222222%\n",
      "Estimators:  5\n",
      "37.03703703703704%\n",
      "Estimators:  10\n",
      "39.351851851851855%\n",
      "Estimators:  1\n",
      "41.66666666666667%\n",
      "Estimators:  10\n",
      "43.98148148148148%\n",
      "Estimators:  20\n",
      "46.2962962962963%\n",
      "Estimators:  5\n",
      "48.61111111111111%\n",
      "Estimators:  20\n",
      "50.92592592592593%\n",
      "Estimators:  1\n",
      "53.24074074074075%\n",
      "Estimators:  10\n",
      "55.55555555555556%\n",
      "Estimators:  1\n",
      "57.870370370370374%\n",
      "Estimators:  5\n",
      "60.18518518518518%\n",
      "Estimators:  20\n",
      "62.5%\n",
      "Estimators:  5\n",
      "64.81481481481481%\n",
      "Estimators:  10\n",
      "67.12962962962963%\n",
      "Estimators:  1\n",
      "69.44444444444444%\n",
      "Estimators:  10\n",
      "71.75925925925925%\n",
      "Estimators:  20\n",
      "74.07407407407408%\n",
      "Estimators:  5\n",
      "76.38888888888889%\n",
      "Estimators:  20\n",
      "78.70370370370371%\n",
      "Estimators:  1\n",
      "81.01851851851852%\n",
      "Estimators:  10\n",
      "83.33333333333334%\n",
      "Estimators:  1\n",
      "85.64814814814815%\n",
      "Estimators:  5\n",
      "87.96296296296296%\n",
      "Estimators:  20\n",
      "90.27777777777779%\n",
      "Estimators:  5\n",
      "92.5925925925926%\n",
      "Estimators:  10\n",
      "94.9074074074074%\n",
      "Estimators:  1\n",
      "97.22222222222221%\n",
      "Estimators:  10\n",
      "99.53703703703704%\n",
      "Estimators:  20\n",
      "9 gini 20 sqrt\n"
     ]
    }
   ],
   "source": [
    "max_depths = [3, 4, 5, 6, 7, 8, 9, 10, 25]\n",
    "criterions = [\"entropy\", \"gini\"]\n",
    "n_estimators = [1, 5, 10, 20]\n",
    "max_features = [\"log2\", \"sqrt\", None]\n",
    "hyperparameter_list = list(product(max_depths, criterions, n_estimators, max_features))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "min_mse = float(\"inf\")\n",
    "counter = 0\n",
    "N = len(hyperparameter_list)\n",
    "for max_depth, criterion, n_estimator, max_feature in hyperparameter_list:\n",
    "    if counter % 5 == 0:\n",
    "        print(f\"{counter/N*100}%\")\n",
    "        print(\"Estimators: \", n_estimator)\n",
    "    counter += 1\n",
    "    val_scores = []\n",
    "    forest = RandomForest(n_estimators=n_estimator, max_depth=max_depth, criterion=criterion, max_features=max_feature)\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        forest.fit(X_train_fold, y_train_fold)\n",
    "        val_score = metrics.mean_squared_error(y_val_fold, forest.predict(X_val_fold))\n",
    "        val_scores.append(val_score)\n",
    "    cv_score = np.mean(val_scores)\n",
    "    if cv_score < min_mse:\n",
    "        min_mse = cv_score\n",
    "        best_max_depth = max_depth\n",
    "        best_criterion = criterion\n",
    "        best_n_estimator = n_estimator\n",
    "        best_max_feature = max_feature\n",
    "\n",
    "print(best_max_depth, best_criterion, best_n_estimator, best_max_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "# forest = RandomForest(n_estimators=best_max_depth, max_depth=best_max_depth, criterion=best_criterion, max_features=best_max_feature)\n",
    "forest = RandomForest(n_estimators=20, max_depth=best_max_depth, criterion=best_criterion, max_features=best_max_feature)\n",
    "forest.fit(X_train, y_train)\n",
    "print(f\"Training accuracy: {metrics.accuracy_score(y_train, forest.predict(X_train))}\")\n",
    "print(f\"Validation accuracy: {metrics.accuracy_score(y_test, forest.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "Estimators:  1\n",
      "2.314814814814815%\n",
      "Estimators:  5\n",
      "4.62962962962963%\n",
      "Estimators:  20\n",
      "6.944444444444445%\n",
      "Estimators:  5\n",
      "9.25925925925926%\n",
      "Estimators:  10\n",
      "11.574074074074074%\n",
      "Estimators:  1\n",
      "13.88888888888889%\n",
      "Estimators:  10\n",
      "16.203703703703702%\n",
      "Estimators:  20\n",
      "18.51851851851852%\n",
      "Estimators:  5\n",
      "20.833333333333336%\n",
      "Estimators:  20\n",
      "23.14814814814815%\n",
      "Estimators:  1\n",
      "25.462962962962965%\n",
      "Estimators:  10\n",
      "27.77777777777778%\n",
      "Estimators:  1\n",
      "30.09259259259259%\n",
      "Estimators:  5\n",
      "32.407407407407405%\n",
      "Estimators:  20\n",
      "34.72222222222222%\n",
      "Estimators:  5\n",
      "37.03703703703704%\n",
      "Estimators:  10\n",
      "39.351851851851855%\n",
      "Estimators:  1\n",
      "41.66666666666667%\n",
      "Estimators:  10\n",
      "43.98148148148148%\n",
      "Estimators:  20\n",
      "46.2962962962963%\n",
      "Estimators:  5\n",
      "48.61111111111111%\n",
      "Estimators:  20\n",
      "50.92592592592593%\n",
      "Estimators:  1\n",
      "53.24074074074075%\n",
      "Estimators:  10\n",
      "55.55555555555556%\n",
      "Estimators:  1\n",
      "57.870370370370374%\n",
      "Estimators:  5\n",
      "60.18518518518518%\n",
      "Estimators:  20\n",
      "62.5%\n",
      "Estimators:  5\n",
      "64.81481481481481%\n",
      "Estimators:  10\n",
      "67.12962962962963%\n",
      "Estimators:  1\n",
      "69.44444444444444%\n",
      "Estimators:  10\n",
      "71.75925925925925%\n",
      "Estimators:  20\n",
      "74.07407407407408%\n",
      "Estimators:  5\n",
      "76.38888888888889%\n",
      "Estimators:  20\n",
      "78.70370370370371%\n",
      "Estimators:  1\n",
      "81.01851851851852%\n",
      "Estimators:  10\n",
      "83.33333333333334%\n",
      "Estimators:  1\n",
      "85.64814814814815%\n",
      "Estimators:  5\n",
      "87.96296296296296%\n",
      "Estimators:  20\n",
      "90.27777777777779%\n",
      "Estimators:  5\n",
      "92.5925925925926%\n",
      "Estimators:  10\n",
      "94.9074074074074%\n",
      "Estimators:  1\n",
      "97.22222222222221%\n",
      "Estimators:  10\n",
      "99.53703703703704%\n",
      "Estimators:  20\n",
      "9 gini 10 sqrt\n"
     ]
    }
   ],
   "source": [
    "max_depths = [3, 4, 5, 6, 7, 8, 9, 10, 25]\n",
    "criterions = [\"entropy\", \"gini\"]\n",
    "n_estimators = [1, 5, 10, 20]\n",
    "max_features = [\"log2\", \"sqrt\", None]\n",
    "hyperparameter_list = list(product(max_depths, criterions, n_estimators, max_features))\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "min_mse = float(\"inf\")\n",
    "counter = 0\n",
    "N = len(hyperparameter_list)\n",
    "for max_depth, criterion, n_estimator, max_feature in hyperparameter_list:\n",
    "    if counter % 5 == 0:\n",
    "        print(f\"{counter/N*100}%\")\n",
    "        print(\"Estimators: \", n_estimator)\n",
    "    counter += 1\n",
    "    val_scores = []\n",
    "    forest = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth, criterion=criterion, max_features=max_feature)\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        forest.fit(X_train_fold, y_train_fold)\n",
    "        val_score = metrics.mean_squared_error(y_val_fold, forest.predict(X_val_fold))\n",
    "        val_scores.append(val_score)\n",
    "    cv_score = np.mean(val_scores)\n",
    "    if cv_score < min_mse:\n",
    "        min_mse = cv_score\n",
    "        best_max_depth = max_depth\n",
    "        best_criterion = criterion\n",
    "        best_n_estimator = n_estimator\n",
    "        best_max_feature = max_feature\n",
    "\n",
    "print(best_max_depth, best_criterion, best_n_estimator, best_max_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9914285714285714\n",
      "Validation accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=best_max_depth, max_depth=best_max_depth, criterion=best_criterion, max_features=best_max_feature)\n",
    "forest.fit(X_train, y_train)\n",
    "print(f\"Training accuracy: {metrics.accuracy_score(y_train, forest.predict(X_train))}\")\n",
    "print(f\"Validation accuracy: {metrics.accuracy_score(y_test, forest.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns names: ['Aroma', 'Flavor', 'Aftertaste', 'Acidity', 'Body', 'Balance', 'Uniformity', 'Sweetness']\n",
      "Target column name: CountryofOrigin\n",
      "X shape: (419, 8)\n",
      "y shape: (419,)\n",
      "[[ 8.17  8.25  8.17 ...  8.17 10.   10.  ]\n",
      " [ 7.75  7.92  7.83 ...  8.17 10.   10.  ]\n",
      " [ 8.    8.    8.   ...  8.   10.   10.  ]\n",
      " ...\n",
      " [ 6.5   6.67  6.42 ...  6.5   8.   10.  ]\n",
      " [ 6.92  7.    6.83 ...  6.92  6.   10.  ]\n",
      " [ 7.08  6.83  6.25 ...  6.75 10.   10.  ]]\n",
      "Training accuracy: 0.9556313993174061\n",
      "Validation accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(\"datasets/coffee_data.csv\", delimiter=\",\", dtype=float, names=True)\n",
    "\n",
    "feature_names = list(data.dtype.names[:-1])\n",
    "target_name = data.dtype.names[-1]\n",
    "\n",
    "X = np.array([data[feature] for feature in feature_names]).T\n",
    "y = data[target_name].astype(int)\n",
    "\n",
    "print(f\"Feature columns names: {feature_names}\")\n",
    "print(f\"Target column name: {target_name}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(X)\n",
    "\n",
    "seed = 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "forest = RandomForest(n_estimators=20, max_depth=best_max_depth, criterion=best_criterion, max_features=best_max_feature)\n",
    "forest.fit(X_train, y_train)\n",
    "print(f\"Training accuracy: {metrics.accuracy_score(y_train, forest.predict(X_train))}\")\n",
    "print(f\"Validation accuracy: {metrics.accuracy_score(y_test, forest.predict(X_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<random_forest.RandomForest object at 0x131d5ad50>' (type <class 'random_forest.RandomForest'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m151\u001b[39m)),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m)),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgini\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(forest, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_squared_error)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest params\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest score:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_score_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_search.py:812\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    809\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    810\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[0;32m--> 812\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[1;32m    816\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    817\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    818\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    824\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    825\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:76\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39m__sklearn_clone__()\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:97\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m    102\u001b[0m             )\n\u001b[1;32m    104\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m    105\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<random_forest.RandomForest object at 0x131d5ad50>' (type <class 'random_forest.RandomForest'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : list(range(1, 151)),\n",
    "    'max_depth' : list(range(1, 51)),\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_features' : ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(forest, param_grid, cv=5, n_jobs=-1, verbose=2, scoring=metrics.mean_squared_error)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'best params{grid_search.best_params_}')\n",
    "print(f'best score:{grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF264",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
